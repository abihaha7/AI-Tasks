{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NU6LRTY4vQF"
      },
      "source": [
        "Backpropagation Implementation in Python for XOR Problem\n",
        "\n",
        "This code demonstrates how backpropagation is used in a neural network to solve the XOR problem. The neural network consists of:\n",
        "\n",
        "1. Defining Neural Network\n",
        "\n",
        "Input layer with 2 inputs\n",
        "\n",
        "Hidden layer with 4 neurons\n",
        "\n",
        "Output layer with 1 output neuron\n",
        "\n",
        "Using Sigmoid function as activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JrxLlQF4o_q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)\n",
        "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
        "        self.bias_hidden = np.zeros((1, self.hidden_size))\n",
        "        self.bias_output = np.zeros((1, self.output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def feedforward(self, X):\n",
        "        self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = self.sigmoid(self.hidden_activation)\n",
        "\n",
        "        self.output_activation = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.predicted_output = self.sigmoid(self.output_activation)\n",
        "\n",
        "        return self.predicted_output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        output_error = y - self.predicted_output\n",
        "        output_delta = output_error * self.sigmoid_derivative(self.predicted_output)\n",
        "\n",
        "        hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
        "        hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate\n",
        "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate\n",
        "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.feedforward(X)\n",
        "            self.backward(X, y, learning_rate)\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LAIhYPR5Knl"
      },
      "source": [
        "def __init__(self, input_size, hidden_size, output_size):: constructor to initialize the neural network\n",
        "\n",
        "self.input_size = input_size: stores the size of the input layer\n",
        "\n",
        "self.hidden_size = hidden_size: stores the size of the hidden layer\n",
        "\n",
        "self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size):\n",
        "initializes weights for input to hidden layer\n",
        "\n",
        "self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)\n",
        ": initializes weights for hidden to output layer\n",
        "\n",
        "self.bias_hidden = np.zeros((1, self.hidden_size)): initializes bias for hidden\n",
        "layer\n",
        "\n",
        "self.bias_output = np.zeros((1, self.output_size)): initializes bias for output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyEaFfnY5gU3"
      },
      "source": [
        "2. Defining Feed Forward Network\n",
        "\n",
        "In Forward pass inputs are passed through the network activating the hidden and output layers using the sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC46xNtE5Y5P"
      },
      "outputs": [],
      "source": [
        "    def feedforward(self, X):\n",
        "          self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden\n",
        "          self.hidden_output = self.sigmoid(self.hidden_activation)\n",
        "\n",
        "          self.output_activation = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "          self.predicted_output = self.sigmoid(self.output_activation)\n",
        "\n",
        "          return self.predicted_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HuE6ngP5tW7"
      },
      "source": [
        "self.hidden_activation = np.dot(X, self.weights_input_hidden) + self.bias_hidden: calculates activation for hidden layer\n",
        "\n",
        "self.hidden_output = self.sigmoid(self.hidden_activation): applies activation function to hidden layer\n",
        "\n",
        "self.output_activation = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output: calculates activation for output layer\n",
        "\n",
        "self.predicted_output = self.sigmoid(self.output_activation): applies activation function to output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqyliQop59MN"
      },
      "source": [
        "3. Defining Backward Network\n",
        "\n",
        "In Backward pass (Backpropagation) the errors between the predicted and actual outputs are computed. The gradients are calculated using the derivative of the sigmoid function and weights and biases are updated accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TTkWCtS54ZF"
      },
      "outputs": [],
      "source": [
        "    def backward(self, X, y, learning_rate):\n",
        "          output_error = y - self.predicted_output\n",
        "          output_delta = output_error * self.sigmoid_derivative(self.predicted_output)\n",
        "\n",
        "          hidden_error = np.dot(output_delta, self.weights_hidden_output.T)\n",
        "          hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "          self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate\n",
        "          self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "          self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate\n",
        "          self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLh4Wxzw6DZ8"
      },
      "source": [
        "output_error = y - self.predicted_output: calculates the error at the output layer\n",
        "\n",
        "output_delta = output_error * self.sigmoid_derivative(self.predicted_output): calculates the delta for the output layer\n",
        "\n",
        "hidden_error = np.dot(output_delta, self.weights_hidden_output.T): calculates the error at the hidden layer\n",
        "\n",
        "hidden_delta = hidden_error * self.sigmoid_derivative(self.hidden_output): calculates the delta for the hidden layer\n",
        "\n",
        "self.weights_hidden_output += np.dot(self.hidden_output.T, output_delta) * learning_rate: updates weights between hidden and output layers\n",
        "\n",
        "self.weights_input_hidden += np.dot(X.T, hidden_delta) * learning_rate: updates weights between input and hidden layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_u9U48E6LlU"
      },
      "source": [
        "4. Training Network\n",
        "\n",
        "The network is trained over 10,000 epochs using the backpropagation algorithm with a learning rate of 0.1 progressively reducing the error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RJRRkLT6RwL"
      },
      "outputs": [],
      "source": [
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.feedforward(X)\n",
        "            self.backward(X, y, learning_rate)\n",
        "            if epoch % 4000 == 0:\n",
        "                loss = np.mean(np.square(y - output))\n",
        "                print(f\"Epoch {epoch}, Loss:{loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQCtV6UI6Z6H"
      },
      "source": [
        "output = self.feedforward(X): computes the output for the current inputs\n",
        "\n",
        "self.backward(X, y, learning_rate): updates weights and biases using backpropagation\n",
        "\n",
        "loss = np.mean(np.square(y - output)): calculates the mean squared error (MSE) loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkd7iAMH6jRW"
      },
      "source": [
        "5. Testing Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r00-kadD6kOM",
        "outputId": "e57f85e1-33e0-4a42-b174-54cd4b11bf39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.2670\n",
            "Epoch 1000, Loss: 0.2185\n",
            "Epoch 2000, Loss: 0.1334\n",
            "Epoch 3000, Loss: 0.0325\n",
            "Epoch 4000, Loss: 0.0125\n",
            "Epoch 5000, Loss: 0.0071\n",
            "Epoch 6000, Loss: 0.0048\n",
            "Epoch 7000, Loss: 0.0036\n",
            "Epoch 8000, Loss: 0.0028\n",
            "Epoch 9000, Loss: 0.0023\n",
            "Predictions after training:\n",
            "[[0.03847731]\n",
            " [0.95554079]\n",
            " [0.95549253]\n",
            " [0.04826057]]\n"
          ]
        }
      ],
      "source": [
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "output = nn.feedforward(X)\n",
        "print(\"Predictions after training:\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJhGGdkk8wkc",
        "outputId": "a770d6d7-f9a4-4e81-b6f0-06ee1fa4c4cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 0.3053\n",
            "Epoch 1000, Loss: 0.2442\n",
            "Epoch 2000, Loss: 0.2104\n",
            "Epoch 3000, Loss: 0.1508\n",
            "Epoch 4000, Loss: 0.0497\n",
            "Epoch 5000, Loss: 0.0194\n",
            "Epoch 6000, Loss: 0.0107\n",
            "Epoch 7000, Loss: 0.0071\n",
            "Epoch 8000, Loss: 0.0052\n",
            "Epoch 9000, Loss: 0.0040\n",
            "\n",
            "Predictions after training:\n",
            "[[0.03559979]\n",
            " [0.94999532]\n",
            " [0.93538288]\n",
            " [0.07148302]]\n"
          ]
        }
      ],
      "source": [
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # XOR problem\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "    nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "    predictions = nn.feedforward(X)\n",
        "    print(\"\\nPredictions after training:\")\n",
        "    print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mGxNAY689Ce"
      },
      "source": [
        "Advantages of Backpropagation for Neural Network Training\n",
        "\n",
        "The key benefits of using the backpropagation algorithm are:\n",
        "\n",
        "Ease of Implementation: Backpropagation is beginner-friendly requiring no prior neural network knowledge and simplifies programming by adjusting weights with error derivatives.\n",
        "\n",
        "Simplicity and Flexibility: Its straightforward design suits a range of tasks from basic feedforward to complex convolutional or recurrent networks.\n",
        "Efficiency: Backpropagation accelerates learning by directly updating weights based on error especially in deep networks.\n",
        "\n",
        "Generalization: It helps models generalize well to new data improving prediction accuracy on unseen examples.\n",
        "\n",
        "Scalability: The algorithm scales efficiently with larger datasets and more complex networks making it ideal for large-scale tasks.\n",
        "\n",
        "Challenges with Backpropagation\n",
        "\n",
        "While backpropagation is powerful it does face some challenges:\n",
        "\n",
        "Vanishing Gradient Problem: In deep networks the gradients can become very small during backpropagation making it difficult for the network to learn. This is common when using activation functions like sigmoid or tanh.\n",
        "Exploding Gradients: The gradients can also become excessively large causing the network to diverge during training.\n",
        "\n",
        "Overfitting: If the network is too complex it might memorize the training data instead of learning general patterns.\n",
        "\n",
        "Backpropagation is a technique that makes neural network learn. By propagating errors backward and adjusting the weights and biases neural networks can gradually improve their predictions. Though it has some limitations like vanishing gradients many techniques like ReLU activation or optimizing learning rates have been developed to address these issues."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
